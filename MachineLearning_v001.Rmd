---
title: "Machine Learning Notes"
author: "davegoblue"
date: "May 17, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background and Overview  
This document is to test a few ideas from the Machine Learning course by Ng (Stanford) on Coursera.  The course seems to dive deeper in to some of the algorithms and linear algebra underlying the techniques.  While the Ng course uses Octave (more or less GNU MATLAB), this document tries to capture a few key concepts in R.  

## Key Concepts  
###_Cost Functions and Linear Regression_  
The course begins with a discussion of cost functions and moving all variables simultaneously in the direction of "greatest descent" in cost.  Generically, suppose you have the following:  
  
* There are m observations each consisting of n predictors and 1 outcome  
* Y - known results in training set (mx1 matrix, commonly known as m-dimensional vector)  
* x1..n - known predictors in training set (mxn matrix, created from the n m-dimensional predictor vectors)  
* X - total predictors in training set (mx(n+1) matrix, where "column 0"" is added as all 1)  
* Theta - potential fitting parameters ((n+1)-dimensional vector matching to the n+1 columns in X)  
  
Then, for any given set of values, define the cost function J for linear regression to be:  
  
* J = [ 1 / (2 * m) ] * sum[ (X * Theta - Y)^2 ]  
* Note that X is the mx(n+1) matrix, so Theta needs to be treated (by transpose or other means) as an (n+1)x1 matrix so that X * Theta becomes an mx1 matrix (or m-dimensional vector) that is comparable to Y  
  
Now, consider just theta(i) which operates on the x(i) column of the X matrix, and how to find its partial derivative.  Expanding:  
  
* J = [ 1 / (2 * m) ] * sum[ (X * Theta)^2 - 2 * (X * Theta) * Y + Y^2 ]  
* Temporarily, define d(X * Theta)/d(thetai) as Q  
* dJ/d(thetai) = [ 1 / (2 * m) ] * sum[2 * (X * Theta) * Q - 2 * Y * Q]  
* dJ/d(thetai) = 1/m * sum[(X * Theta - Y) * Q]  
* And, d(X * Theta) / d(thetai) = x(i)  
* So, dJ/d(thetai) = 1/m * sum[(X * Theta - Y) * x(i)]  
  
The cost function algorithm then includes a learning factor, alpha.  This factor determines how far to run in the "opposite direction, same magnitude"" of each partial derivative w.r.t theta.  
  
We begin with a very simple dataset, mtcars, looking only at the continuous variables (mpg, disp, hp, drat, wt, and qsec):  
```{r}
library(caret, quietly=TRUE)  # May be useful for confusionMatrix and the like
data(mtcars)
str(mtcars)

varPreds <- mtcars[,c("disp", "hp", "drat", "wt", "qsec")]
vecMPG <- mtcars$mpg
vecOnes <- rep(1, nrow(varPreds))

mtxX <- as.matrix(cbind(vecOnes, varPreds))
mtxY <- as.matrix(vecMPG)
mtxTheta <- matrix(data=1, nrow=ncol(mtxX), ncol=1)

str(mtxY)
str(mtxX)
str(mtxTheta)
```
  
The dimensions all look OK, and we will have 6 theta (5 predictors plus the intercept) across 32 observations.  Next, we run the simple linear regression to see what the coefficients "should" be:  
```{r}
mpgLM <- lm(vecMPG ~ ., data=varPreds)  # Includes intercept by R default, only a -1 kills that off
summary(mpgLM)
```
  
While this is far from the world's best regression, it solves since all of the underlying matrices are non-singular.  We can test the coefficients using the relevant matrix math, specifically:  
  
* Optimal Theta = solve(X' * X) * X' * Y, where X' is the transponse of X and solve() is the matrix inverse  
  
We test this matrix math on the same data:  
```{r}
optTheta <- solve(t(mtxX) %*% mtxX) %*% t(mtxX) %*% mtxY
print(round(t(optTheta),4))
print(round(summary(mpgLM)$coef[,1],4))
```
  
The coefficients are identical, as expected given that matrix math underlies the lm algorithm in R.  
  
Lastly, we test the gradient descent method using the same dataset, trying several alpha:  
```{r}
gdTest <- function(gdX=mtxX, gdY=mtxY, alpha=0.001, nIter=20000, mtxCalc=TRUE) {
    nObs <- nrow(gdX)  # Total number of observations . . .  
    mtxTheta <- matrix(0, nrow=ncol(gdX), ncol=1)  # Set up a theta matrix
    
    # Standardize the X variables to be roughly -1 to 1
    # Calculate the mean and SD for each column
    meanX <- apply(gdX, 2, FUN=mean)
    sdX <- apply(gdX, 2, FUN=sd)
    
    # Set the mean/sd for column 1 (all ones) to be 0/1 so we do not change it
    meanX[1] <- 0  # Use 0 as the mean conversion factor for column 1
    sdX[1] <- 1  # Use 1 as the sd scaling factor for column 1
    
    # Use vector operations to update gdX (probably a better way, but I do not yet know it)
    gdX <- (gdX - mapply(rep, meanX, nObs)) / mapply(rep, sdX, nObs)
    
    # Set up the cost function vectors, and initialize with preliminary data
    vecCost <- rep(NA, nIter+1)
    vecCost[1] <- (0.5 / nObs) * sum((gdX %*% mtxTheta - gdY)^2)
    vecPartial <- rep(NA, ncol(gdX))
    
    for (intCtr in 1:nIter) {
        # Simultaneously update thetas for the next iteration
        if (mtxCalc) {
            # Use matrix operations to simultaneously update theta
            # theta = theta - (alpha/nObs) * Q
            # Q should be the vector of partial derivatives
            mtxPartial <- t(gdX) %*% (gdX %*% mtxTheta - gdY)  # Matrix of (# theta)x1
            mtxTheta <- mtxTheta - (alpha/nObs) * mtxPartial  # Matrix of (# theta)x1
        } else {
            # Run using a for loop instead - same answers, but much slower
            # Added as an example for benchmarking purposes
            # Need to create all the partial derivatives (loop 1) then apply them (loop 2)
            for (varNum in 1:ncol(gdX)) {
                vecPartial[varNum] <- sum((gdX %*% mtxTheta - gdY) * gdX[,varNum])
            }
            for (varNum in 1:ncol(gdX)) {
                mtxTheta[varNum, 1] <- mtxTheta[varNum, 1] - (alpha/ nObs) * vecPartial[varNum]
            }
        }
        
        # Update the cost vector, using slot intCtr + 1
        vecCost[intCtr + 1] <- (0.5 / nObs) * sum((gdX %*% mtxTheta - gdY)^2)
    }
    
    plot(vecCost, main=paste0("Cost Function vs. Iterations: ",alpha))
    print(paste0("Intercept: ",round(mtxTheta[1,1],4)))
    print(round(mtxTheta[-1,1] / sdX[-1],4))
    
    return(gdX %*% mtxTheta)
}

baseTime <- proc.time()

y0001 <- gdTest(alpha=0.0001)
y0003 <- gdTest(alpha=0.0003)
y0010 <- gdTest(alpha=0.001)
y0030 <- gdTest(alpha=0.003)
y0100 <- gdTest(alpha=0.01)
y0300 <- gdTest(alpha=0.03)
y1000 <- gdTest(alpha=0.1)
y3000 <- gdTest(alpha=0.3)

print("Total run time for function gdTest: ")
print(proc.time() - baseTime)

# Benchmarking based on subset of mtcars dataset
# mtxX is 32x6 (32 observations, each with an intercept plus 5 predictors)
# mtxY is 32x1 (32 results)
# nIter is 20,000
# 8 combinations of alpha are run, all that converge so there are no NaN or related timing issues
# 
# Run using matrix maths to simultaneosuly update theta:  4.3 user 2.1 system 6.9 elapsed
# RMSE from different alpha [lm() RMSE is 2.306]: 3.675 2.398 2.320 2.306 2.306 2.306 2.306 2.306
# 
# Run using for loops to simultaneously update theta:  14.1 user 2.1 system 17.7 elapsed
# RMSE from different alpha [lm() RMSE is 2.306]: 3.675 2.398 2.320 2.306 2.306 2.306 2.306 2.306
# 
# So, even with these very small datasets, matrix maths reduce user time by ~70% and elapsed time by ~60%
```
  
The gradient descents converge at different paces depending on the learning parameter (alpha).  For a look at the goodness of fit, we assess the actual predictions:  
```{r}
graphFrame <- cbind(y0001[,1], y0003[,1], y0010[,1], y0030[,1], 
                    y0100[,1], y0300[,1], y1000[,1], y3000[,1], 
                    mtcars$mpg
                    )
graphFrame <- graphFrame[order(graphFrame[,ncol(graphFrame)]),]
plot(x=1:nrow(graphFrame), y=graphFrame[,ncol(graphFrame)], type="l", 
     col="dark green", lwd=3, ylim=c(0,50)
     )
lines(x=1:nrow(graphFrame), y=graphFrame[,1], col="red", lty=2)
lines(x=1:nrow(graphFrame), y=graphFrame[,4], col="blue", lty=2)
lines(x=1:nrow(graphFrame), y=graphFrame[,6], col="purple", lty=2)
lines(x=1:nrow(graphFrame), y=graphFrame[,8], col="orange", lty=2)
```

As expected, the smaller alpha have not yet had enough time to converge, and so they have worse predictions (their gradients are still declining).  On the other hand, for the larger alpha, they have had enough time to converge without inducing oscillations, suggesting that the are a reasonable solution to the problem.  To assess how good, we examine:  
```{r}
rmseLM <- sqrt(mean((summary(mpgLM)$residuals)^2))
rmseMod <- rep(0, ncol(graphFrame)-1)
for (intCtr in 1:(ncol(graphFrame)-1)) {
    rmseMod[intCtr] <- sqrt(mean((graphFrame[,ncol(graphFrame)] - graphFrame[,intCtr])^2))
}

print(paste0("RMSE from LM: ",round(rmseLM,3)))
print(paste0("RMSE from Gradient Descent: "))
print(round(rmseMod,3))
```
  
Gradient descent eventually finds coefficients that are close to optimal for RMSE.  This exploration is helpful for seeing some advantnges of gradient descent (no matrix inverting means it can be fast for many predictors, no per se requirement that the cost function be linear), as well as some advantages of the closed form (converges immediately to the optimal answer with no iterations, easier to program).  This will be a good topic for continued exploration.  
  

###_Logistic Regression_  
Logistic regression ("logit") is used to make binary classifications.  By convention, 1 often expresses the presence of something (malignnancy, school acceptance, fraudulent transaction, etc.) while 0 is set to express the absence of that something.  The models work the same regardless of which state is called 1, of course.  
  
There are many drivers behind using logit rather than linear regression for a classification problem.  Among the drivers:  
  
* New data can play a large role even when they obviously do not matter.  For example, if we model tumor size vs. mailgnancy, we may find 2.5 cm is roughly where most tumors start to be malignant.  We might even get an OK linear regression line that passes through a prediction of 0.5 at ~2.5 cm.  But, if we add malignancies with 10 cm, the regression line will pass through 0.5 at a larger tumor size.  This makes little sense as our model had already learned that 2.5 cm is the cutoff; it became stupider when exposed to additional data that agreed with it!  
* The linear regression will also classify some points as being below 0 probability and above 1 probability.  While this is not the end of the world (we will predict <0 as 0 and >1 as 1), neither is it sensible.  

An example of this phenomenon is shown below, based on entirely made up data:  
```{r}
set.seed(1605181831)

par(mfrow=c(1, 2))

## Create some fake tumor data
tumorSize <- runif(100, min=0, max=0.1)
tumorType <- rbinom(length(tumorSize), 1, 1 - ((0.1 - tumorSize)^2 * 100))
fakeTumorLMOrig <- lm(tumorType ~ tumorSize)
keyCutOrig <- (0.5-coef(fakeTumorLMOrig)[[1]])/coef(fakeTumorLMOrig)[[2]]
predOrig <- pmax(0, pmin(1, round(predict(fakeTumorLMOrig, data.frame(tumorSize)), 0) 
                         ) 
                 )

plot(tumorSize, tumorType, xlab="Tumor Size", ylab="Malignant (1=Yes, 0=No)", col=1+predOrig, pch=19,
     main=paste0("50% Probability at Tumor Size: ",round(keyCutOrig,4)), cex.main=0.85
     )
abline(fakeTumorLMOrig, col="blue", lwd=2)
abline(h=0.5, v=keyCutOrig, lty=2, lwd=2)

confOrig <- confusionMatrix(predOrig, tumorType, positive="1")
text(0.04, 0.4, paste0("Accuracy: ",round(confOrig$overall[["Accuracy"]], 2)), adj=c(0, 0))
text(0.04, 0.34, paste0("Sensitivity: ",round(confOrig$byClass[["Sensitivity"]], 2)), adj=c(0, 0))
text(0.04, 0.28, paste0("Specificity: ",round(confOrig$byClass[["Specificity"]], 2)), adj=c(0, 0))
text(0.04, 0.22, paste0("+ Pred Val: ",round(confOrig$byClass[["Pos Pred Value"]], 2)), adj=c(0, 0))
text(0.04, 0.16, paste0("- Pred Val: ",round(confOrig$byClass[["Neg Pred Value"]], 2)), adj=c(0, 0))

## Add some larger tumors that are all malignant
tumorSizeMod <- c(tumorSize, runif(50, min=0.1, max=0.2))
tumorTypeMod <- c(tumorType, rep(1, 50))
fakeTumorLMMod <- lm(tumorTypeMod ~ tumorSizeMod)
keyCutMod <- (0.5-coef(fakeTumorLMMod)[[1]])/coef(fakeTumorLMMod)[[2]]
predMod <- pmax(0, pmin(1, round(predict(fakeTumorLMMod, data.frame(tumorSizeMod)), 0) 
                        ) 
                )

plot(tumorSizeMod, tumorTypeMod, xlab="Tumor Size", ylab="Malignant (1=Yes, 0=No)", col=1+predMod, pch=19,
     main=paste0("50% Probability at Tumor Size: ",round(keyCutMod,4)), cex.main=0.85
     )
abline(fakeTumorLMMod, col="blue", lwd=2)
abline(h=0.5, v=keyCutMod, lty=2, lwd=2)

confMod <- confusionMatrix(predMod, tumorTypeMod, positive="1")
text(0.04, 0.4, paste0("Accuracy: ",round(confMod$overall[["Accuracy"]], 2)), adj=c(0, 0))
text(0.04, 0.34, paste0("Sensitivity: ",round(confMod$byClass[["Sensitivity"]], 2)), adj=c(0, 0))
text(0.04, 0.28, paste0("Specificity: ",round(confMod$byClass[["Specificity"]], 2)), adj=c(0, 0))
text(0.04, 0.22, paste0("+ Pred Value: ",round(confMod$byClass[["Pos Pred Value"]], 2)), adj=c(0, 0))
text(0.04, 0.16, paste0("- Pred Value: ",round(confMod$byClass[["Neg Pred Value"]], 2)), adj=c(0, 0))

par(mfrow=c(1,1))

```
  
The LM appropriately responds to the new data by driving up the intercept (more of the data are positive, and making more positive predictions will drive up the overall accuracy).  But, this means we declare tumors of size 0.011 to have a 50% chance of being malignant, even though the closed has probability of malignancy at size 0.011 as ~20%.  
  
This can be seen from the very poor specificity of the modified test; we essentially declare everyone to have cancer, so a positive result from our test does not have much meaning (prior would not change by that much.
  
Logistic regression is an attempt to cure some of these deficiencies.  Broadly, differences in the logit include:  
  
* Predicted probability, h, in the logit is 1 / [ 1 + exp(-(X * theta)) ], as opposed to X * theta in linear regression.  The 1 / (1 + exp(-d)) is referred to as the sigmoid function, which has the desirable properties that it asymptotes to 0 for d of -Inf and to 1 for d of +Inf; it also is 0.5 for d=0, so making predictions can be as easy as just looking at each predicted d [-(X * theta) ] and using 0 as the cutoff  
* The cost function is defined based on the "true y" to be -log(h) for "true y == 1" and -log(1-h) for "true y == 0".  Essentially, the more extreme (whether towards 0 or towards 1) the probability decalred by the model, the greater the penalty for a missed prediction.  
* This cost function is asserted to be convex, while using sum((y-x)^2) as in linear regression is asserted to have many local minima.  Further, this cost function is asserted to be equivalent to a maximum likelihood estimate (MLE).  
* The short version seems to be that a great deal of thought (and proof) went in to this cost function, making it a good modelling choice  
  
The logit model is now tested on the same fake malignancy data as used in the previous example.  The call for the logit in R is based on glm() using family="binomial".  The prediction method is broadly the same, with an added call to type="response":  
```{r}
par(mfrow=c(1, 2))

## Run the GLM (logit) for the original tumor data
fakeTumorGLMOrig <- glm(tumorType ~ tumorSize, family="binomial")

## When response variables times coefficients equal intercept, h=0.5 (sigmoid(0))
keyCutOrigGLM <- -coef(fakeTumorGLMOrig)[[1]]/coef(fakeTumorGLMOrig)[[2]]
predOrigGLM <- pmax(0, pmin(1, round(predict(fakeTumorGLMOrig, data.frame(tumorSize), type="response"), 0) 
                           ) 
                    )

plot(tumorSize, tumorType, xlab="Tumor Size", ylab="Malignant (1=Yes, 0=No)", col=1+predOrigGLM, pch=19,
     main=paste0("50% Probability at Tumor Size: ",round(keyCutOrigGLM,4)), cex.main=0.85
     )
points(x=tumorSize, y=predict(fakeTumorGLMOrig, data.frame(tumorSize), type="response"), col="blue", pch=19)
abline(h=0.5, v=keyCutOrigGLM, lty=2, lwd=2)

confOrigGLM <- confusionMatrix(predOrigGLM, tumorType, positive="1")
text(0.04, 0.4, paste0("Accuracy: ",round(confOrigGLM$overall[["Accuracy"]], 2)), adj=c(0, 0))
text(0.04, 0.34, paste0("Sensitivity: ",round(confOrigGLM$byClass[["Sensitivity"]], 2)), adj=c(0, 0))
text(0.04, 0.28, paste0("Specificity: ",round(confOrigGLM$byClass[["Specificity"]], 2)), adj=c(0, 0))
text(0.04, 0.22, paste0("+ Pred Val: ",round(confOrigGLM$byClass[["Pos Pred Value"]], 2)), adj=c(0, 0))
text(0.04, 0.16, paste0("- Pred Val: ",round(confOrigGLM$byClass[["Neg Pred Value"]], 2)), adj=c(0, 0))

## Run the logit on the dataset with the larger tumors that are all malignant
fakeTumorGLMMod <- glm(tumorTypeMod ~ tumorSizeMod, family="binomial")

## When response variables times coefficients equal intercept, h=0.5 (sigmoid(0))
keyCutModGLM <- -coef(fakeTumorGLMMod)[[1]]/coef(fakeTumorGLMMod)[[2]]
predModGLM <- pmax(0, pmin(1, round(predict(fakeTumorGLMMod, data.frame(tumorSizeMod), type="response"), 0) 
                        ) 
                )

plot(tumorSizeMod, tumorTypeMod, ylab="Malignant (1=Yes, 0=No)", 
     col=1+predModGLM, pch=19, cex.main=0.85, xlab="Tumor Size", 
     main=paste0("50% Probability at Tumor Size: ",round(keyCutModGLM,4))
     )
points(y=predict(fakeTumorGLMMod, data.frame(tumorSizeMod), type="response"), 
       x=tumorSizeMod, col="blue", pch=19
       )
abline(h=0.5, v=keyCutModGLM, lty=2, lwd=2)

confModGLM <- confusionMatrix(predModGLM, tumorTypeMod, positive="1")
text(0.04, 0.4, paste0("Accuracy: ",round(confModGLM$overall[["Accuracy"]], 2)), adj=c(0, 0))
text(0.04, 0.34, paste0("Sensitivity: ",round(confModGLM$byClass[["Sensitivity"]], 2)), adj=c(0, 0))
text(0.04, 0.28, paste0("Specificity: ",round(confModGLM$byClass[["Specificity"]], 2)), adj=c(0, 0))
text(0.04, 0.22, paste0("+ Pred Value: ",round(confModGLM$byClass[["Pos Pred Value"]], 2)), adj=c(0, 0))
text(0.04, 0.16, paste0("- Pred Value: ",round(confModGLM$byClass[["Neg Pred Value"]], 2)), adj=c(0, 0))

par(mfrow=c(1,1))

```
  
While the overall accuracy of the logit is actually slightly lower (with the added malignancy data), the model is in aggregate substantially better.  Specifically, the model on the original data already tuned to predict 99%+ probabilities at sizes of 0.10 and above.  So, when the model gets a whack of new data that is all 100% malignant at sizes of 0.10 - 0.20, it does not learn anything new (it is, for practical purposes, unchanged).  As such, the model remains just as good at predicting negatives while getting better (technically, just seeing even more obvious cases of) at predicting positives.  It retains its specificity.  
  
It is notable that for "ideal" faked data (the original 100 data points), the LM slightly outperformed the logit.  The underlying data has structure that is neither precisely p ~ x nor precisely p ~ 1/(1+exp(-x))) so neither of these formulations precisely matches the data.  As always, the question during analysis is which model better reflects realtiy and better predicts unseen cases.  There is nothing magical about the logit that makes it certain to outperform simply because the outcomes are all 0/1.  But, it is a decent default with some very attractive properties.  
  
###_Regularization_  
The concept of regularization is that it can be helpful to shrink the magnitudes of the coefficients, largely to avoid overfitting to the training data.  The specific implementation is that that sum-squared-error for the linear regression becomes sum((f(x) - y)^2) + lambda * sum(theta^2), where by convention the intercept term is not included in the theta^2 summation.  
  
In general, regularization means worse predictions on the training data, since the method preferences small increases in RMSE if achieved by holding down the magnitude of the parameters.  However, the method can increase test/validation accuracy materially by vastly reducing variance (overfitting of the training data).  

A classic case for regularization is conofunding input vectors correlated to a relevant input vector.  While we "know" the correct model is Y = x1 + eps, if x2/x3 ~ x1 the regression can find something silly like Y = -2 * x1 + 4 * x2 - 1 * x3 + eps based on a small structure that happens to appear in the randomness in the training data.  The small gain in RMSE is completely artificial, and can fail in the validation stage.  For example:  
```{r}
set.seed(1605200803)
nSim <- 40

x1 <- rnorm(nSim)  #x1 = N(0, 1)
x2 <- 0.5 * x1 + rnorm(nSim, 0, 0.01)  # x2 will be 0.5 * x1 + N(0, 0.01)
x3 <- 2 * x1 + rnorm(nSim, 0, 0.4)  # x3 will be 2 * x1 + N(0, 0.4)
y <- x1 + rnorm(nSim)  # y will be x1 + N(0, 1)

# Test how well y = x1 fits the data - what is the R^2?
rndTSS <- sum( (y - mean(y))^2 )
rndSSE <- sum( (y - x1)^2 )
rndR2 <- 1 - rndSSE / rndTSS
print(paste0("R-squared for an assumed model of y = x1 is: ", round(rndR2,4)))

# Run the actual regression using lm
rndLM <- lm(y ~ x1 + x2 + x3)
summary(rndLM)
```

The full linear regression improves R-squared (unadjsuted) from 0.5255 to 0.5866, but at the expense of wildly varying thetas (+13.6 vs. +1.0 and -22.3 vs. 0.0).  It is reasonable to doubt this model will perform as well on an independent validation set - let's see:  
```{r}
x1Test <- rnorm(nSim)  #x1Test = N(0, 1)
x2Test <- 0.5 * x1Test + rnorm(nSim, 0, 0.01)  # x2Test will be 0.5 * x1Test + N(0, 0.01)
x3Test <- 2 * x1Test + rnorm(nSim, 0, 0.4)  # x3Test will be 2 * x1Test + N(0, 0.4)
yTest <- x1Test + rnorm(nSim)  # yTest will be x1Test + N(0, 1)

# Test how well y = x1 fits the test data - what is the R^2?
rndTestTSS <- sum( (yTest - mean(yTest))^2 )
rndTestSSE <- sum( (yTest - x1Test)^2 )
rndTestR2 <- 1 - rndTestSSE / rndTestTSS
print(paste0("R-squared when applying y = x1 to test data is: ", 
             round(rndTestR2,4)
             )
      )

# Test how well the lm fits the data
rndPredLM <- predict(rndLM, data.frame(x1=x1Test, x2=x2Test, x3=x3Test))
lmTestSSE <- sum( (yTest - rndPredLM)^2 )
print(paste0("R-squared when applying lm(y ~ x1 + x2 + x3) to test data is: ", 
             round(1-lmTestSSE/rndTestTSS, 4)
             )
      )
```

Absent regularization, the LM performs worse than the assumed y=x1.  While this is a contrived example, it provides an excellent illustration of how attempting to reduce bias with additional predictors can instead explode variance.  The "more accurate" training model then performs materially worse on validation data.  

A further issue in this example is that there is very high VIF in all of the LM coefficients, meaning that the model is rather unstable to small perturbations.  So, the coefficients and predictions not only have higher variance, they are additionally much harder to interpret.  See plots:  
```{r}
par(mfrow=c(1, 2))

plot(x=x1, y=y, pch=19, col="blue", main="Training data")
abline(a=0, b=1, lwd=2, lty=2, col="orange")
points(x=x1, y=predict(rndLM, data.frame(x1=x1, x2=x2, x3=x3)), 
       pch=19, col="red", cex=0.75
       )

plot(x=x1Test, y=yTest, pch=19, col="blue", main="Testing data")
abline(a=0, b=1, lwd=2, lty=2, col="orange")
points(x=x1Test, y=rndPredLM, pch=19, col="red", cex=0.75)

par(mfrow=c(1, 1))
```
  
The red points represent an overfit to the training data, as they are (essentially) tuning on some noise in x2 and x3 while trying to keep x1 + x2 + x3 = 1 in line with the actual trend of the data (x1=1, x2=x3=0).  When the overfit is applied to the testing data, it is not certain to perform well; in this example, it is actually performing worse.  
  
The LM can instead be run using regularization, and there are many algorithms (ridge, lasso, elasticnet, etc.) coded in R.  For this demonstration, we use lm.ridge() from the MASS library:  
```{r}
library(MASS)

# Reuse the same y, x1, x2, x3 from above (35 lambda runs)
myLambda = c(seq(0, 0.1, by=0.01), seq(0.12, 0.30, by=.02), seq(0.35, 1.00, by=.05))
rndLMRidge <- lm.ridge(y ~ x1 + x2 + x3, lambda=myLambda)

par(mfrow=c(1, 2))

# Plot the coefficients vs. Lambda
plot(y=coef(rndLMRidge)[,2], x=myLambda, type="l", col="blue", lwd=2,
     ylim=c(floor(min(coef(rndLMRidge))), ceiling(max(coef(rndLMRidge)))),
     main="Ridge Coefficients", xlab="Lambda", ylab="Coefficients"
     )
lines(y=coef(rndLMRidge)[,3], x=myLambda, lwd=2, col="dark green")
lines(y=coef(rndLMRidge)[,4], x=myLambda, lwd=2, col="purple")
abline(h=0, lty=2)
legend("bottomright", legend=c("x1", "x2", "x3"), col=c("blue", "dark green", "purple"), lwd=2)

# Calculate and then plot the R^2 for Test and Training Data
mtxOrig <- matrix(data=c(rep(1, nSim), x1, x2, x3), ncol=4, byrow=FALSE)
mtxTest <- matrix(data=c(rep(1, nSim), x1Test, x2Test, x3Test), ncol=4, byrow=FALSE)
mtxY <- matrix(data=rep(y, length(myLambda)), nrow=nSim, byrow=FALSE)
mtxYTest <- matrix(data=rep(yTest, length(myLambda)), nrow=nSim, byrow=FALSE)
mtxCoefRidge <- as.matrix(coef(rndLMRidge))

predRidgeOrig <- mtxOrig %*% t(mtxCoefRidge)  # nSimx4 %*% 4x11 = nSimx11 (each y predicted 11 times)
predRidgeTest <- mtxTest %*% t(mtxCoefRidge)  # nSimx4 %*% 4x11 = nSimx11 (each y predicted 11 times)

r2OrigRidge <- 1 - (colSums((predRidgeOrig - mtxY)^2) / rndTSS)
r2TestRidge <- 1 - (colSums((predRidgeTest - mtxYTest)^2) / rndTestTSS)

plot(x=myLambda, y=r2OrigRidge, type="l", col="blue", lwd=2, ylim=c(0.3, 0.7),
     main="R^2 LM.RIDGE", xlab="Lambda", ylab="R^2"
     )
lines(x=myLambda, y=r2TestRidge, col="dark green", lwd=2, ylim=c(0.3 ,0.7))
legend("topright", legend=c("Train Data", "Test Data"), col=c("blue", "dark green"), lwd=2)

par(mfrow=c(1, 1))
```
  
* As lambda increases, the magnitude of the coefficients decreases and the accuracy on the training data decreases also (as expected)  
* But, accuracy on the test (validation) data increases with lambda, showing that regularization is helping the model to avoid overfitting to the training data  
* At some point, lambda could become so large that the model goes to intercept-only, so it is not the case that larger lambda is always better  
* Calculating the best lambda would likely require k-fold and/or bootstrap to assess lambda with optimal predictive power, for example as implemented by caret::train  
* This example was simply to illustrate the that penalizing large coefficients can, under some conditions, improve out-of-sample predictive power  
  
###_Log Loss vs RMSE Example for Binomial Classification_  
The loss function for the logistic regression is set to highly penalize wrong predictions made with high confidence.  Specifically:  
  
* Consider h(x) to be the predicted probability given vector x, associated to y which is the actual 0/1    
* RMSE Cost (per estimate): (y - h(x))^2  
* Log-Loss Cost (per estimate): [ -y * log(h(x)) ] - [ (1 - y) * log(1 - h(x)) ]  
  
We can graph these functions for y=0 and y=1:  
```{r}

pEst <- seq(0.01, 0.99, by=0.01)

par(mfrow=c(1, 2))


# Plot RMSE Loss Functions
plot(x=pEst, y=(1-pEst)^2, type="l", col="blue", lwd=2, main="RMSE for 0/1 Class",
     xlab="Estimated Probability", ylab="RMSE Cost"
     )
lines(x=pEst, y=(0-pEst)^2, type="l", col="dark green", lwd=2)
abline(h=0, v=0.5, lty=2)
legend("top", legend=c("True y=1", "True y=0"), col=c("blue", "dark green"), lwd=2)

# Plot Log Loss Functions
plot(x=pEst, y=-log(pEst), type="l", col="blue", lwd=2, main="Log-Loss for 0/1 Class",
     xlab="Estimated Probability", ylab="Log-Loss Cost"
     )
lines(x=pEst, y=-log(1-pEst), type="l", col="dark green", lwd=2)
abline(h=0, v=0.5, lty=2)
legend("top", legend=c("True y=1", "True y=0"), col=c("blue", "dark green"), lwd=2)


par(mfrow=c(1, 1))

```
  
A related question is what prediction is optimal if you "know" the true probability to be p.  Under both RMSE and Log-Loss, any estimate other than p will incur higher errors (on average):  
  
* Suppose that true probability is p, and estimated probability is d  
* Over n trials, this experiment should on average have p * n state "1"" and (1-p) * n state "0"  
  
* RMSE E[Cost]:  (p * n) * (1 - d)^2 + ((1 - p) * n) * (-d)^2  
* RMSE E[Cost]:  (p * n) * (1 - 2 * d + d^2) + ((1 - p) * n) * d^2  
* RMSE E[Cost]:  (p * n) - 2 * (p * n) * d + (p * n) * d^2 + n * d^2 - p * n * d^2  
* d(RMSE E[Cost])/dd:  -2 * p * n + 2 * (p * n) * d + 2 * d * n - 2 * (p * n) * d  
* d(RMSE E[Cost])/dd:  -2 * p * n + 2 * d * n, which equals 0 (hits minimum) when d=p  
  
* Log-Loss E[Cost]:  -(p * n) * ln(d) - (1 - p) * n * ln(1 - d)  
* d(Log-Loss E[Cost])/dd:  -(p * n) / d + (1 - p) * n / (1 - d)  
* d(Log-Loss E[Cost])/dd:  Need (1 - p) * n / (1 - d) = (p * n) / d -> (1 - p) * n * d = (1 - d) * (p * n)
* d(Log-Loss E[Cost])/dd:  Need n * d  - (p * n) * d = (p * n) - (p * n) * d -> n * d = n * p -> d = p  
  
###_One vs. All Classification_  
Sometimes the feature for a classification problem has more than two potential states.  For example, a patient could be healthy, sick with sore throat, or sick with flu.  Provided that the states are MECE, then the typical logit approach can be run once for each state, with the feature being state="yes" vs. state="no".  
  
In many ways, the pre-processing of features is analogous to what would be required for a chi-squared test of association:  
  
* If some observations do not fall in to any of the states, create an additional state called "None"  
* If some observations fall in to 2+ of the states, either create additional states (A only, B only, A and B, None) or declare that each of the observations has a primary state (e.g., perhaps sore throat and flu should be flagged as flu and not sore throat).  The choice of feature creation/aggregation may materially influence the model's performance.  
  
The data for this example is from an assignment downloaded for the Coursera Ng Machine Learning class.  The Ng course uses Octave (free version of MATLAB), and the data are on my machine in MATLAB format.  The function readMat() from library R.matlab is used to read this data for processing in R.  This code is cached so that the data need only be converted to R format once.
  
```{r, cache=TRUE}
library(R.matlab, quietly=TRUE)

matPath <- "../../../OctaveDirectory/Week04/"  # Using relative path
matData <- "ex3data1.mat"

listMatData <- readMat(paste0(matPath, matData))
str(listMatData)  # Check that as expected - list with two items, $X and $y

mtxX <- listMatData$X  # First item in the list should be 5000x400
mtxY <- listMatData$y  # Second item in the list (y) should be 5000x1

str(mtxX)  # Check that 5000x400 and matricized
str(mtxY)  # Check that 5000x1 and matricized

hist(mtxY[,1], col="blue", breaks=0:10)  # Equal distributions of items in each of y==1:10
```

The MATLAB data have loaded successfully, and mtxY has 500 observations in each class 1:10.  There are 400 potential predictors for each of the 5000 observations, so there is some risk of over-fitting.  There are some clever matrix maths that can be run (see later), but for now we run through 10 trials using glmnet().  The glmnet() function will test mutliple ambda using elasticnet regularization.  

Many default parameters are used, with the exception that we ask not to standaridze the input variables (they are already pixel activations 0-1 so there is no major outlier concern).  Further, we request alpha=0 (ridge penbalty) rather than the default alpha=1 (lasso penalty).  This is a logit (family="binomial") regression call.  Further, a for loop is used (clunkily) to go through each of the 10 cases, with the results cached since this is not an overwhelmingly fast procedure:  
```{r, cache=TRUE}
library(glmnet, quietly=TRUE)

glmList <- vector("list", 10)

for (intCtr in 1:10) {
    modelY=factor(mtxY[,1]==intCtr, levels=c(FALSE, TRUE))
    
    # Run ridge (alpha=0) regressions using logit (family="binomial") using defaults for lambda search
    glmList[[intCtr]] <- glmnet(x=mtxX, y=modelY, family="binomial", standardize=FALSE, alpha=0)
}

```

Just as an example, we inspect the predictions made by this routine, thinking specifically about how well it does at predicting 5 vs. not 5:  
```{r}
# If this is cached, then RStudio has no access to glmnet -- duplicate call here
library(glmnet, quietly=TRUE)

# Request the ridge penalty (alpha=0), same as modeled
# Request s=0.01 (nearest result to lambda=0.01)
myTest <- predict(glmList[[5]], mtxX, type="response", s=0.00595, alpha=0)

plot(myTest, col= 1 + (mtxY[,1] == 5),
     main="Prediction for y=5", ylab="Probability", xlab="Index"
     )
```
  
Then, run the predictions for each of 1-10, pick the maximum, and report/graph the accuracies:  
```{r}
probY <- matrix(data=-1, nrow=nrow(mtxY), ncol=10)

for (intCtr in 1:10) {
    probY[,intCtr] <- predict(glmList[[intCtr]], mtxX, type="response", s=0.01, alpha=0)
}

predY <- apply(probY, 1, FUN=which.max)

plot(x=1:length(predY), y=predY, col = 1 + (mtxY[,1] != predY), 
     main="Predictions vs Actual", ylab="Prediction (Red=Wrong)", xlab="Index"
     )

confusionMatrix(predY, mtxY[,1])
```

In this case, the simple model (with guessed lambda, no cross-validation, and no out-of-sample prediction) classifies ~90% of the images correctly.  This code will later work through better methods for fitting matrices () since a simple Octave routine drives 95% accuracy without too much added time.  The idea was merely to show that the concept of logit classification can be extended to mutliple classes as needed.  
  
###_Neural Networks - Representation and Forward Propagation_  
Neural networks are a fairly old idea that experienced a resurgence as computing power became cheaper and more readily available.  The general idea is to find non-linear features in the input data that combine to be useful predictors.  While creating thousands (or even millions) of new covariates using combinations of polynomials on the original predictors is also a possibility, the neural network can drive strong accuracy without needing to make so many guesses (and then process/filter them down) at the covariate creation stage.  
  
Broadly, the idea of the neural network is to function like the brain.  Input signals excite various combinations of neurons, which then pass messages to other neurons, which eventually reach a conclusion.  The idea is to try to make the computer behave like the brain.  
  
A small bit of the science is that neurons are cells in the brain.  Broadly, neurons get information by way of dendrites, make some calculations, and pass results on to other neurons by way of axons.  The neurons sometimes work as a group, and scientists have showed that they can be re-wired.  For example, people can "see" by way of their tongue or use reflected sounds (sonar) to navigate if they have enough training and repetition.  
  
For the computer, the process works by taking input data, processing it through one or more hidden layers, and then creating a final hypothesis.  In general for classification, the sigmoid (1 / (1 + exp(-x)) ) function is applied to the result at each stage.  As well, a bias term (all 1s) is introduced for the next step of the process.  Suppose we have input matrix X (n observatons of m covariates) and reponse vector y (n observations of factors):  
  
* Step 1:  Create a column of all ones at the front of X, and call this X1  
* Step 2a:  Create a2 = g(X1 * Theta1)  
* Step 2b:  Add a column of all ones at the front of a2, and call this X2  
* Step 3a:  Create a3 = g(X2 * Theta2)  
* Step 3b:  Add a column of all ones at the front, and call this X3  
* . . . 
* Step Final:  Create aFinal = g(Xfinal-1 * Thetafinal-1), and call this the prediction  
  
The intermediate nodes are referred to as "hidden layers".  While it is common to have only a single "hidden layer", sometimes it is helpful to have more.  
  
As a somewhat simplified example, a small hidden layer can perform any of the common logical functions, as well as XOR (exclusive OR) and XNOR (negation of exclusive XOR):  
  
* AND: Suppose you have x0 (bias term) of 1 with binary inputs x1 and x2.  Then, if you have theta = c(-30, 20, 20) you will get g(-30) ~ 0 for x1=x2=0, g(-10) ~ 0 for x1+x2=1, and g(10) ~ 1 for x1=x2=1  
* OR:  Same concept, but with theta = c(-10, 20, 20), providing g(-10) ~ 0 for x1=x2=0 and g(10) ~ 1 or g(30) ~ 1 for x1+x2=1 or x1=x2=1  
* NEGATION: Same idea but with just x1 and theta = c(10, -20)  
* XOR:  Create hidden node1 as NOT AND using theta = c(30, -20, -20) and hidden node2 as OR using theta = c(-10, 20, 20).  Then add the bias unit, and use final theta = c(-30, 20, 20) which will come back TRUE provided that both OR and NOT AND are true  
  
As you continue to add nodes/layers, you can continue to add logic and interactions and the like.  The output vector can also be a multi-class rather than just a single 0/1.  
  
For an example, we expand on the number recognition problem from the previous section.  The instructor calculated a theta matrix that can be loaded from the MATLAB format:  
```{r}
# If above has been cached, then RStudio has no access to R.matlab -- duplicate call here
library(R.matlab, quietly=TRUE)

matPath <- "../../../OctaveDirectory/Week04/"  # Using relative path
wgtData <- "ex3weights.mat"

listWgtData <- readMat(paste0(matPath, wgtData))
str(listWgtData)  # Check that as expected - list with two items, $Theta1 and $Theta2

mtxTheta1 <- listWgtData$Theta1  # First item in the list should be 25x401
mtxTheta2 <- listWgtData$Theta2  # Second item in the list (y) should be 10x26

str(mtxTheta1)  # Check that 25x401 and matricized
str(mtxTheta2)  # Check that 10x26 and matricized

```
  
These data can then be used to run the forward propagation, ending with a 5000x10 matrix containing probabilities for each of the 10 potential states.  
  
```{r}
XOnes <- cbind(rep(1, nrow(mtxX)), mtxX)  # Will now be 5000x401

mtxA2 <- XOnes %*% t(mtxTheta1)  # 5000x401 %*% 401x25 will be 5000x25
mtxA2 <- 1 / (1 + exp(-mtxA2))  # sigmoid (R assumes non-matrix math unless requested otherwise)

a2Ones <- cbind(rep(1, nrow(mtxA2)), mtxA2)  # Will now be 5000x26

mtxResults <- a2Ones %*% t(mtxTheta2)  # 5000x26 %*% 26x10 will be 5000x10 (as desired)
mtxResults <- 1 / (1 + exp(-mtxResults))  # sigmoid (R assumes non-matrix math unless requested otherwise)

```

Further, the above code for finding the best prediction (maximum probability) and plotting the outcomes is slightly adapted:  
```{r}
predYnn <- apply(mtxResults, 1, FUN=which.max)

plot(x=1:length(predYnn), y=predYnn, col = 1 + (mtxY[,1] != predYnn), 
     main="Predictions vs Actual", ylab="Prediction (Red=Wrong)", xlab="Index"
     )

confusionMatrix(predYnn, mtxY[,1])

```
  
With these well-optimized theta (more on that later), the neural network achieves ~97.5% accuracy on the in-sample data.  We have not yet run any out-of-sample testing.  
  
###_Neural Networks - Logic Gates_  
Below is an example of using neural networks to create logic gates for two input nodes.  We create the data such that there is a bias unit (1) and then each of the four binary combinations of a1/a2:  
```{r}
nnLogicInput <- matrix(data=c(1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0), nrow=4, byrow=FALSE)
nnLogicInput

nnThetaAnd <- c(-30, 20, 20)
nnThetaOr <- c(-10, 20, 20)
nnThetaNotA <- c(10, -20, 0)

nnThetaOne <- matrix(data=c(nnThetaAnd, nnThetaOr, nnThetaNotA), nrow=3, byrow=FALSE)

nnHiddenOne <- cbind(rep(1, 4), 1 / (1 + exp(-(nnLogicInput %*% nnThetaOne))))
round(nnHiddenOne, 3)

nnThetaXOR <- c(-10, -20, 20, 0)
nnThetaXNOR <- c(10, 20, -20, 0)
nnThetaA <- c(10, 0, 0, -20)
nnThetaB <- c(-30, 20, 20, 20)
nnKeepAnd <- c(-10, 20, 0, 0)
nnKeepOr <- c(-10, 0, 20, 0)

nnThetaTwo <- matrix(data=c(nnThetaXOR, nnThetaXNOR, nnKeepAnd, nnKeepOr, nnThetaA, nnThetaB), 
                     nrow=4, byrow=FALSE
                     )

nnHiddenTwo <- 1 / (1 + exp(-(nnHiddenOne %*% nnThetaTwo)))
prtHidden <- cbind(nnLogicInput[,-1], nnHiddenTwo)
dimnames(prtHidden)[[2]] <- c("Raw A", "Raw B", "XOR", "XNOR", "AND", "OR", "Proc A", "Proc B")
round(prtHidden, 3)
```
  
So while this is a highly simplified example, it illustrated the power of the neural network to tease out interactions among the inputs.  
  
###_Neural Networks - Back Propagation_
####_Back Propagation (Setup and Costs)_  
Back propagation is a technique for optimizing Thetas in a neural network.  The process broadly runs as follows:  
  
* Start with randomized Theta matrices (there is a singularity issue if there are any initial symmetries)  
* Forward propagate the train X data by way of the Theta matrices to make predictions  
* Adjudge the predictions for delta (desired change), and back-propagate the deltas from end to start  
* Adjust all Theta matrices accordingly, and then repeat  
  
Like many things, it is less complicated to write than to implement.  But, the image-recognition from above can be leveraged for this analysis to see if we can come up with "good" theta choices on our own.  First, we double-check that the X and Y data are still available:  
```{r}
str(mtxX)  # Check that 5000x400 and matricized
str(mtxY)  # Check that 5000x1 and matricized

hist(mtxY[,1], col="blue", breaks=0:10)  # Check equal distributions of items in each of y==1:10
```
  
Next, we create images of a random sample of digits and initialize theta matrices assuming a 25-unit hidden layer with 10 possible outcomes:  
```{r}
nInputLayer <- ncol(mtxX)
nHiddenNodes <- 25
nFinalLabels <- 10
epsInit <- 0.12  # Initial thetas will be randomized as epsInit * runif(min=-1, max=1)

# First matrix needs [1 + nInputLayer] columns and nHiddenNodes rows
bpThetaOne <- matrix(data=runif(nHiddenNodes * (1 + nInputLayer), min=-epsInit, max=epsInit), 
                     nrow=nHiddenNodes, ncol=(1 + nInputLayer), byrow=FALSE
                     )

# Second matrix needs [1 + nHiddenNodes] columns and nFinalLables rows
bpThetaTwo <- matrix(data=runif(nFinalLabels * (1 + nHiddenNodes), min=-epsInit, max=epsInit), 
                     nrow=nFinalLabels, ncol=(1 + nHiddenNodes), byrow=FALSE
                     )

# Set up margins and mfrow for image printing
defMargin <- par()$mar
par(mar=c(0, 0, 0, 0))
par(mfrow=c(8, 12))

# Grab 96 random images to show
myVal <- sample(5000, 96, replace=FALSE)

# Draw the images (need to manipulate a lot of up vs. down due to how image() works)
for (picNum in myVal) { 
    image( matrix(data=rev(mtxX[picNum,]), nrow=20, byrow=TRUE)[20:1,], axes=FALSE) 
}

# Print the associated mtxY for comparison
print(matrix(data=mtxY[myVal,1], nrow=8, byrow=TRUE))

# Put graphing back to defaults
par(mfrow=c(1,1))
par(mar=defMargin)
```

Next, we need to convert matrices to vectors for passing back/forth to various cost and gradient functions.  Essentially, the idea is that minimization functions want to work on vectors while we actually have matrices.  So, we will convert matrices to vectors, use functions to convert back to matrices to calculate cost and gradients, then pass back the results as numbers and vectors.  The first phase of this is to create a cost function that takes a single vector of ThetaOne/ThetaTwo plus descriptors and calculates the cost.  We then unroll the matrices to vectors and run the cost calculation (forward propagation) function:  
```{r}
nnCost <- function(vecMatrix, nInputLayer, nHiddenNodes, nFinalLabels, X, y, lambda) {
    
    # Roll vecMatrix back to theta matrices (assume Theta One by rows then Theta Two by rows)
    keyCut <- (nInputLayer + 1) * nHiddenNodes
    mtxOneData <- vecMatrix[1:keyCut]
    mtxTwoData <- vecMatrix[(keyCut+1):length(vecMatrix)]
    useThetaOne <- matrix(data=mtxOneData, nrow=nHiddenNodes, ncol=(nInputLayer + 1), byrow=FALSE)
    useThetaTwo <- matrix(data=mtxTwoData, nrow=nFinalLabels, ncol=(1 + nHiddenNodes), byrow=FALSE)
    
    # Forward propagate using X, y, and thetas
    m = nrow(X)
    Xones <- cbind(rep(1, m), X)
    layerOne <- cbind(rep(1, m), (1 / (1 + exp(-(Xones %*% t(useThetaOne))) ) ) )
    layerTwo <- 1 / (1 + exp(-(layerOne %*% t(useThetaTwo))) ) 
    
    # Calculate costs using log-loss, assuming the match should be to column number
    yPreds <- (y[,1] == matrix(data=rep(1:nFinalLabels, m), nrow=m, ncol=nFinalLabels, byrow=TRUE))
    nnJ <- (-1/m) * ( sum(yPreds * log(layerTwo)) + sum((1-yPreds) * log(1 - layerTwo) ) )
    
    # Add the lambda costing also
    nnJ <- nnJ + (lambda / (2 * m)) * (sum(useThetaOne[,-1]^2) + sum(useThetaTwo[,-1]^2) )
    
    return(nnJ)
}

```
  
The function is then tested on the instructor inputs as provided in exercise 4 from Ng Machine Learning on Coursera:  
```{r}
# If above has been cached, then RStudio has no access to R.matlab -- duplicate call here
library(R.matlab, quietly=TRUE)

matPath <- "../../../OctaveDirectory/Week05/"  # Using relative path
wgtData <- "ex4weights.mat"

listWgtData <- readMat(paste0(matPath, wgtData))
str(listWgtData)  # Check that as expected - list with two items, $Theta1 and $Theta2

testTheta1 <- listWgtData$Theta1  # First item in the list should be 25x401
testTheta2 <- listWgtData$Theta2  # Second item in the list (y) should be 10x26

str(testTheta1)  # Check that 25x401 and matricized
str(testTheta2)  # Check that 10x26 and matricized

# Calculate cost for lambda=0 (should be 0.287629)
nnCost(vecMatrix=c(as.vector(testTheta1), as.vector(testTheta2)), nInputLayer=nInputLayer, 
       nHiddenNodes=nHiddenNodes, nFinalLabels=nFinalLabels, X=mtxX, y=mtxY, lambda=0
       )

# Calculate cost for lambda=1 (should be 0.383770)
nnCost(vecMatrix=c(as.vector(testTheta1), as.vector(testTheta2)), nInputLayer=nInputLayer, 
       nHiddenNodes=nHiddenNodes, nFinalLabels=nFinalLabels, X=mtxX, y=mtxY, lambda=1
       )

```
  
Further, a very short function is written to calculate the sigmoid gradient for any object (number, vector, matrix, etc.) that is passed to it.  The maths are:  
  
* Sigmoid = 1 / (1 + exp(-z))  
* dSigmoid/dz = exp(-z) / (1 + exp(-z))^2 = (1 - Sigmoid) * Sigmoid  
* Desire is to perform this calculation for each element, assuming that z has been passed to the function    
  
As well, an even shorter function is written to calculate the Sigmoid, g(z), given an input z:  
```{r}
sigmoidGradient <- function(zSig) {
    tempSig <- 1 / (1 + exp(-zSig))
    return(tempSig * (1 - tempSig))
}

sigmoidPlain <- function(zSig) {
    return(1 / (1 + exp(-zSig)))
}

sigmoidPlain(0)
sigmoidPlain(c(1, -0.5, 0, 0.5, 1))
sigmoidPlain(matrix(data=0.5*(0:14-7), nrow=5, byrow=FALSE))

sigmoidGradient(0)
sigmoidGradient(c(1, -0.5, 0, 0.5, 1))
sigmoidGradient(matrix(data=0.5*(0:14-7), nrow=5, byrow=FALSE))
```
  
####_Back Propagation (Gradients)_  
The back-propagation algorithm is at the heart of the neural network setup.  The intent is to find the gradients for every entry in the theta matrices by back-propagating the error in each training example.  Specifically, given this example with a single hidden layer, we attempt:  
  
For each set of inputs x(i) spanning x(1)-x(m), run the following:  
  
1.  Treat x(i) as the sole input to the neural network (the a1 vector).  Use this to forward propagate to z2, a2=g(z2), z3, a3=g(z3); recalling that g() is the sigmoid function  
2.  Calculate the deltas for each of the k final nodes, definining delta(3)(k) as (a(3)(k) - y(k)) or basically how much different is my node than I wish it were  
3.  Calculate the deltas for each of the hidden layer nodes, defined as delta2 = t(ThetaTwo) %*% delta(3) * g'(z(2)), where g' means the sigmoid gradient and z(2) is the hidden layer PRIOR to running sigmoid  
4.  Accumulate deltas as D(L) = D(L) + 1/m * delta(L+1) % * % t(a(L)), where a(L) is the activation of the nodes in layer L while delta(L+1) is the delta from the layer one ahead  
  
Note that by convention, delta and gradient and etc. for the bias unit is always considered to be 0 since the bias unit will always be 1.  This can be accounted for in many ways, but the gist is that it should never be fed to an optimizer function.  

A gradient function is created to manage this.  In theory, it would be nice to add this as a component of the cost function.  For now, it is maintained as a separate function taking the same inputs:  
```{r}
nnGradient <- function(vecMatrix, nInputLayer, nHiddenNodes, nFinalLabels, X, y, lambda) {

    # Copied directly from nnCost for unrolling the Theta vectors
    # Roll vecMatrix back to theta matrices (assume Theta One by rows then Theta Two by rows)
    keyCut <- (nInputLayer + 1) * nHiddenNodes
    mtxOneData <- vecMatrix[1:keyCut]
    mtxTwoData <- vecMatrix[(keyCut+1):length(vecMatrix)]
    useThetaOne <- matrix(data=mtxOneData, nrow=nHiddenNodes, ncol=(nInputLayer + 1), byrow=FALSE)
    useThetaTwo <- matrix(data=mtxTwoData, nrow=nFinalLabels, ncol=(1 + nHiddenNodes), byrow=FALSE)
    
    # Set up empty Theta Gradients
    gradThetaOne <- matrix(data=0, nrow=nHiddenNodes, ncol=(nInputLayer + 1), byrow=FALSE)
    gradThetaTwo <- matrix(data=0, nrow=nFinalLabels, ncol=(1 + nHiddenNodes), byrow=FALSE)
    
    # Calculate the number of observations to process
    m = nrow(X)
    
    # Convert y (mx1) to yPreds (mx10) with each column being "is it this digit (1/0"
    yPreds <- (y[,1] == matrix(data=rep(1:nFinalLabels, m), nrow=m, ncol=nFinalLabels, byrow=TRUE))
    
    # Run the forward propagation outside the loop
    # Step 1 - Forward propagate this observation
    a1 <- cbind(rep(1, m), X)  # Take X and add 1 in front (mx401)
    z2 <- a1 %*% t(useThetaOne)  # mx401 %*% 401x25 = mx25
    a2 <- cbind(rep(1, m), sigmoidPlain(z2))  # Take sigmoid of z2, add 1 in front (mx26)
    z3 <- a2 %*% t(useThetaTwo)  # mx26 %*% 26x10 = mx10
    a3 <- sigmoidPlain(z3)  # Take sigmoid of z3, and this is a3
    
    # Calculate delta03 outside the loop
    # Step 2 - Calculate delta03
    delta_03 <- a3 - yPreds  # This will be mx10, same as a3
    
    # Calculate delta02 outside the loop
    # Step 3 - Calculate delta02
    delta_02 <- (delta_03 %*% useThetaTwo[,-1]) * sigmoidGradient(z2)  # (mx10 %*% 10x25) * mx25 = mx25
    
    # Calculate the gradients outside the for loop
    # Step 4a - Accumulate Theta Gradeints

    # 10x26 + 10x5000 %*% 5000x26 = 10x26 (as desired)
    gradThetaTwo <- gradThetaTwo + t(delta_03) %*% a2

    # 25x401 + 25x5000 %*% 5000x401 = 25x401 (as desired)
    gradThetaOne <- gradThetaOne + t(delta_02) %*% a1    
    
    # Step 4b - Regularize accumulated gradients by 1/m
    gradThetaTwo <- (1/m) * gradThetaTwo
    gradThetaOne <- (1/m) * gradThetaOne
    
    # Roll thetas together in to a long vector for return
    return(c(as.vector(gradThetaOne), as.vector(gradThetaTwo)))    
}
```
  
The next step will be to tie this together through an optimization machine.  
  
####_Back Propagation (Optimize Thetas)_  
At any given point, we have a core set of data, including:  
  
* mtxX - the feature matrix (5000x400) 
* mtxY - the response vector (5000x1)  
* bpThetaOne - the most recent Theta1, starting as uniform -eps to eps (25x401)  
* bpThetaTwo - the most recent Theta2, starting as uniform -eps to eps (10x26)  
* nInputLayer - the number of features (400)  
* nHiddenLayer - the number of hidden layer nodes (25)  
* nFinalLabels - the number of final categories (10)  
* lambda - regularization parameter (can start with 1)  
* vecPass - rolls bpThetaOne and bpThetaTwo in to a single vector (length 10,285) by first going by columns along bpThetaOne and then going by columns along bpThetaTwo  
* nnCost - function that will pass back the current cost  
* nnGradient - function that will pass back the current gradient, in an order that matched up to vecMatrix  
  
In theory, this is enough to optimize the theta matrices.  We begin by writing a short function for the forward propagation (should clean this up in a later version - redundant with code above):  
```{r}
forwardPredY <- function(mtxX, mtxTheta1, mtxTheta2) {
    XOnes <- cbind(rep(1, nrow(mtxX)), mtxX)  # Will now be 5000x401
    
    mtxA2 <- XOnes %*% t(mtxTheta1)  # 5000x401 %*% 401x25 will be 5000x25
    mtxA2 <- 1 / (1 + exp(-mtxA2))  # sigmoid (R assumes non-matrix math unless requested otherwise)
    
    a2Ones <- cbind(rep(1, nrow(mtxA2)), mtxA2)  # Will now be 5000x26
    
    mtxResults <- a2Ones %*% t(mtxTheta2)  # 5000x26 %*% 26x10 will be 5000x10 (as desired)
    mtxResults <- 1 / (1 + exp(-mtxResults))  # sigmoid (R assumes non-matrix math unless requested otherwise)
    
    predYnn <- apply(mtxResults, 1, FUN=which.max)
    
    return(predYnn)
}
```
  
We begin by studying the baseline data, and its associated costs and accuracies:  
```{r}
lambda=1  # Run scenario with lambda=1

vecPass=c(as.vector(bpThetaOne), as.vector(bpThetaTwo))  # Roll together bpThetaOne and bpThetaTwo

# Assess the current accuracies
predYnn <- forwardPredY(mtxX, bpThetaOne, bpThetaTwo)
plot(x=1:length(predYnn), y=predYnn, col = 1 + (mtxY[,1] != predYnn), 
     main="Predictions vs Actual", ylab="Prediction (Red=Wrong)", xlab="Index"
     )
confusionMatrix(predYnn, mtxY[,1])

# Print the current cost
nnCost(vecMatrix=vecPass, nInputLayer, nHiddenNodes, nFinalLabels, X=mtxX, y=mtxY, lambda)

# Plot the current gradients
plot(nnGradient(vecMatrix=vecPass, nInputLayer, nHiddenNodes, nFinalLabels, X=mtxX, y=mtxY, lambda))
```
  
Apparently, the initial predictions using random theta are not good!  Next, we try to optimize theta (by way of associated vecPass) using some of the R built-in functionality:  
```{r, cache=TRUE}
# Run the optimization process
startTime <- proc.time()
myList <- optim(vecPass, nnCost, nnGradient, nInputLayer=nInputLayer,
                nHiddenNodes=nHiddenNodes, nFinalLabels=nFinalLabels,
                X=mtxX, y=mtxY, lambda=lambda, control=list(maxit=150), method="BFGS"
                )
proc.time() - startTime
```
  
The optim() function is seemingly not as fast as the equivalent function fmingc() in Octave, although it is driving better accuracy with more iterations so there may be a chicken and egg issue.  This may be because optim() is not yet tuned, and packages like "trust" and/or further tweaking of control parameters may help.  

Still, it provides a good starting solution to a neural network with ~10,000 parameters for ~50,000 predictions in ~5 minutes.  So, while it can likely be significantly further optimized, it is pretty cool to be able to do simple image identification (on a cleaned dataset consisting of numbers only) in not too much time.  
  
Below, we assess the accuracy of the optimized neural network:  
```{r}
# Convert the result back to Theta matrices
keyCut <- (nInputLayer + 1) * nHiddenNodes
postBPThetaOne <- matrix(data=myList$par[1:keyCut], nrow=nHiddenNodes, byrow=FALSE)
postBPThetaTwo <- matrix(data=myList$par[(keyCut+1):length(myList$par)], nrow=nFinalLabels, byrow=FALSE)

# Assess the optimized accuracies
predYPostnn <- forwardPredY(mtxX, postBPThetaOne, postBPThetaTwo)
plot(x=1:length(predYPostnn), y=predYPostnn, col = 1 + (mtxY[,1] != predYPostnn), 
     main="Predictions vs Actual", ylab="Prediction (Red=Wrong)", xlab="Index"
     )
confusionMatrix(predYPostnn, mtxY[,1])

# Print the post-optimization cost
nnCost(vecMatrix=myList$par, nInputLayer, nHiddenNodes, nFinalLabels, X=mtxX, y=mtxY, lambda)

# Plot the post-optimization gradients
plot(nnGradient(vecMatrix=myList$par, nInputLayer, nHiddenNodes, nFinalLabels, X=mtxX, y=mtxY, lambda))

```
  
The model is ~98% accurate with identifying images (n.b., there was no out-of-sample test run, so this is an inflated accuracy) based on this simple neural network.  A very interesting exercise!  
  
###_Model Selection (Bias vs. Varaince)_  
A primary goal with prediction is to correctly classify out-of-sample observations, meaning data that was not used to train the model.  When applyin a model to make out-of-sample predictions, there are three common sources of error:  
  
* Intrinsic - there is some degree of randomness in the real-world, and no model can ever avoid the impact of that intrinsic randomness on model predictions  
* Bias (under-fitting) - there is some structure in the data that is not captured in the model, thus the model makes worse predictions than it would if it tuned on that structure  
* Variance (over-fitting) - the model has tuned on some noise in the training sample, and will make poor real-world predictions when it no longer can leverage the training noise  
  
To assess and minimize error, data are frequently split in to 2-3 groups prior to modeling:  
  
* Training data (frequently 60%-80% of the total data) - this is the primary data that is used in creating and tuning the model  
* Validation / CV data (optional; if present, frequently 10%-20% of the total data) - this data is used for checking the goodness of various models coming from the training dataset, and then using those findings to further refine the models with the training data.  Some techniques use resampling techniques (k-fold, bootstrap, etc.) that automatically run the CV process with the training data, and in those cases this split might be eliminated  
* Test data (frequently 20%-30% of the data) - this data is used a single time for assessing the quality of the final model.  It should not be used at all for model tuning.  
  
A simple example of bias/variance comes from y = x^2 + eps, where eps ~ N(0, sigma^2).  The predictions are certain to have error-squared of at least sigma-squared, since this is an intrinsic error.  However, suppose that we model:  
  
* y=theta0 + theta1 * x : This model will not find the curvature that is in the data, so it will tend to be a poor fit to the training dataset and will also perform poorly on the CV data and Test data  
* y=theta0 + theta1 * x^1 + theta2 * x^2 + . . . theta50 * x^50 : This model will tune on a lot of the noise in the training data and may make good training predictions; however, these predictions will crater when applied to the CV data and Test data  
  
See for example:  
```{r}
set.seed(2016052618)
x <- runif(150, 0, 4)
y <- x^2 + rnorm(150, 0, 2)  # Add some noise to y = x^2

# Split in to Train and Test (without using caret::createDataPartition())
inTrain <- sample(length(x), round(.7*length(x)), replace=FALSE)

xTrain <- x[inTrain]
xTest <- x[-inTrain]
yTrain <- y[inTrain]
yTest <- y[-inTrain]

# Create all polynomials 1-50
mtxXTrain <- matrix(data=0, nrow=length(xTrain), ncol=50)
mtxXTest <- matrix(data=0, nrow=length(xTest), ncol=50)
for (intCtr in 1:50) {
    mtxXTrain[, intCtr] <- xTrain^intCtr
    mtxXTest[, intCtr] <- xTest^intCtr
}

# Run the biased model
biasLM <- lm(yTrain ~ mtxXTrain[,1])
plot(xTrain, yTrain, type="p", pch=19, col="red", 
     xlab="X (training)", ylab="Y (training)", main="Training Data: Fits for Bias, Proper, Variance"
     )
abline(biasLM, lty=2, lwd=2, col="blue")


# Run the "correct" model
properLM <- lm(yTrain ~ mtxXTrain[,2])
lines(xTrain[order(xTrain)], properLM$fitted.values[order(xTrain)], lty=1, lwd=4, col="dark green")


# Run the variance model
varianceLM <- lm(yTrain ~ mtxXTrain[,])
lines(xTrain[order(xTrain)], varianceLM$fitted.values[order(xTrain)], lty=1, lwd=4, col="purple")

legend("topleft", legend=c("Bias (y=x)", "Proper (y=x^2)", "Variance (y=x^(1:50))"), 
       col=c("blue", "dark green", "purple"), lty=c(2, 1, 1), lwd=c(2, 4, 4)
       )


# Apply to the test data
plot(xTest, yTest, type="p", pch=19, col="red", 
     xlab="X (test)", ylab="Y (test)", main="Testing Data: Fits for Bias, Proper, Variance"
     )

yBiasTest <- coef(biasLM)[[1]] + coef(biasLM)[[2]] * mtxXTest[,1]
lines(xTest[order(xTest)], yBiasTest[order(xTest)], lty=2, lwd=2, col="blue")

yProperTest <- coef(properLM)[[1]] + coef(properLM)[[2]] * mtxXTest[,2]
lines(xTest[order(xTest)], yProperTest[order(xTest)], lty=1, lwd=4, col="dark green")

coefVar <- coef(varianceLM)
coefVar[is.na(coefVar)] <- 0
yVarianceTest <- mtxXTest %*% matrix(data=coefVar[2:length(coefVar)], ncol=1) + coefVar[[1]]
lines(xTest[order(xTest)], yVarianceTest[order(xTest)], lty=1, lwd=4, col="purple")

legend("topleft", legend=c("Bias (y=x)", "Proper (y=x^2)", "Variance (y=x^(1:50))"), 
       col=c("blue", "dark green", "purple"), lty=c(2, 1, 1), lwd=c(2, 4, 4)
       )

print(paste0("RMSE Training (Bias): ", round(sqrt(mean((yTrain - biasLM$fitted.values)^2)),2)))
print(paste0("RMSE Training (Proper): ", round(sqrt(mean((yTrain - properLM$fitted.values)^2)),2)))
print(paste0("RMSE Training (Variance): ", round(sqrt(mean((yTrain - varianceLM$fitted.values)^2)),2)))

print(paste0("RMSE Testing (Bias): ", round(sqrt(mean((yTest - yBiasTest)^2)),2)))
print(paste0("RMSE Testing (Proper): ", round(sqrt(mean((yTest - yProperTest)^2)),2)))
print(paste0("RMSE Testing (Variance): ", round(sqrt(mean((yTest - yVarianceTest)^2)),2)))

```

The bias model struggles in both the training data and the testing data since it cannot tune on the curvature in the underlying data.  The best solution would be to add a covariate to help it match the real-world structure.  
  
The proper model performs reasonably in both the training data and the testing data.  There is not much to be done about the intrinsic noisiness of y.  
  
The variance model performs the best in the training data and the worst in the testing data.  This is common with severe overfits which have been learning about noise in addition to structure.  The best solutions would be to either 1) reduce the number of covariates, 2) add regularization to damp down the coefficients, or 3) get a larger dataset (in which case it is much harder to over-fit).  CV/Test error much worse than Train error is a common symptom of variance.  
  