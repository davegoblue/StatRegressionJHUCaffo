---
title: "Machine Learning Notes"
author: "davegoblue"
date: "May 17, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background and Overview  
This document is to test a few ideas from the Machine Learning course by Ng (Stanford) on Coursera.  The course seems to dive deeper in to some of the algorithms and linear algebra underlying the techniques.  While the Ng course uses Octave (more or less GNU MATLAB), this document tries to capture a few key concepts in R.  

## Key Concepts  
###_Cost Functions and Linear Regression_  
The course begins with a discussion of cost functions and moving all variables simultaneously in the direction of "greatest descent" in cost.  Generically, suppose you have the following:  
  
* There are m observations each consisting of n predictors and 1 outcome  
* Y - known results in training set (mx1 matrix, commonly known as m-dimensional vector)  
* x1..n - known predictors in training set (mxn matrix, created from the n m-dimensional predictor vectors)  
* X - total predictors in training set (mx(n+1) matrix, where "column 0"" is added as all 1)  
* Theta - potential fitting parameters ((n+1)-dimensional vector matching to the n+1 columns in X)  
  
Then, for any given set of values, define the cost function J for linear regression to be:  
  
* J = [ 1 / (2 * m) ] * sum[ (X * Theta - Y)^2 ]  
* Note that X is the mx(n+1) matrix, so Theta needs to be treated (by transpose or other means) as an (n+1)x1 matrix so that X * Theta becomes an mx1 matrix (or m-dimensional vector) that is comparable to Y  
  
Now, consider just theta(i) which operates on the x(i) column of the X matrix, and how to find its partial derivative.  Expanding:  
  
* J = [ 1 / (2 * m) ] * sum[ (X * Theta)^2 - 2 * (X * Theta) * Y + Y^2 ]  
* Temporarily, define d(X * Theta)/d(thetai) as Q  
* dJ/d(thetai) = [ 1 / (2 * m) ] * sum[2 * (X * Theta) * Q - 2 * Y * Q]  
* dJ/d(thetai) = 1/m * sum[(X * Theta - Y) * Q]  
* And, d(X * Theta) / d(thetai) = x(i)  
* So, dJ/d(thetai) = 1/m * sum[(X * Theta - Y) * x(i)]  
  
The cost function algorithm then includes a learning factor, alpha.  This factor determines how far to run in the "opposite direction, same magnitude"" of each partial derivative w.r.t theta.  
  
We begin with a very simple dataset, mtcars, looking only at the continuous variables (mpg, disp, hp, drat, wt, and qsec):  
```{r}
library(caret, quietly=TRUE)  # May be useful for confusionMatrix and the like
data(mtcars)
str(mtcars)

varPreds <- mtcars[,c("disp", "hp", "drat", "wt", "qsec")]
vecMPG <- mtcars$mpg
vecOnes <- rep(1, nrow(varPreds))

mtxX <- as.matrix(cbind(vecOnes, varPreds))
mtxY <- as.matrix(vecMPG)
mtxTheta <- matrix(data=1, nrow=ncol(mtxX), ncol=1)

str(mtxY)
str(mtxX)
str(mtxTheta)
```
  
The dimensions all look OK, and we will have 6 theta (5 predictors plus the intercept) across 32 observations.  Next, we run the simple linear regression to see what the coefficients "should" be:  
```{r}
mpgLM <- lm(vecMPG ~ ., data=varPreds)  # Includes intercept by R default, only a -1 kills that off
summary(mpgLM)
```
  
While this is far from the world's best regression, it solves since all of the underlying matrices are non-singular.  We can test the coefficients using the relevant matrix math, specifically:  
  
* Optimal Theta = solve(X' * X) * X' * Y, where X' is the transponse of X and solve() is the matrix inverse  
  
We test this matrix math on the same data:  
```{r}
optTheta <- solve(t(mtxX) %*% mtxX) %*% t(mtxX) %*% mtxY
print(round(t(optTheta),4))
print(round(summary(mpgLM)$coef[,1],4))
```
  
The coefficients are identical, as expected given that matrix math underlies the lm algorithm in R.  
  
Lastly, we test the gradient descent method using the same dataset, trying several alpha:  
```{r}
gdTest <- function(gdX=mtxX, gdY=mtxY, alpha=0.001, nIter=20000, mtxCalc=TRUE) {
    nObs <- nrow(gdX)  # Total number of observations . . .  
    mtxTheta <- matrix(0, nrow=ncol(gdX), ncol=1)  # Set up a theta matrix
    
    # Standardize the X variables to be roughly -1 to 1
    # Calculate the mean and SD for each column
    meanX <- apply(gdX, 2, FUN=mean)
    sdX <- apply(gdX, 2, FUN=sd)
    
    # Set the mean/sd for column 1 (all ones) to be 0/1 so we do not change it
    meanX[1] <- 0  # Use 0 as the mean conversion factor for column 1
    sdX[1] <- 1  # Use 1 as the sd scaling factor for column 1
    
    # Use vector operations to update gdX (probably a better way, but I do not yet know it)
    gdX <- (gdX - mapply(rep, meanX, nObs)) / mapply(rep, sdX, nObs)
    
    # Set up the cost function vectors, and initialize with preliminary data
    vecCost <- rep(NA, nIter+1)
    vecCost[1] <- (0.5 / nObs) * sum((gdX %*% mtxTheta - gdY)^2)
    vecPartial <- rep(NA, ncol(gdX))
    
    for (intCtr in 1:nIter) {
        # Simultaneously update thetas for the next iteration
        if (mtxCalc) {
            # Use matrix operations to simultaneously update theta
            # theta = theta - (alpha/nObs) * Q
            # Q should be the vector of partial derivatives
            mtxPartial <- t(gdX) %*% (gdX %*% mtxTheta - gdY)  # Matrix of (# theta)x1
            mtxTheta <- mtxTheta - (alpha/nObs) * mtxPartial  # Matrix of (# theta)x1
        } else {
            # Run using a for loop instead - same answers, but much slower
            # Added as an example for benchmarking purposes
            # Need to create all the partial derivatives (loop 1) then apply them (loop 2)
            for (varNum in 1:ncol(gdX)) {
                vecPartial[varNum] <- sum((gdX %*% mtxTheta - gdY) * gdX[,varNum])
            }
            for (varNum in 1:ncol(gdX)) {
                mtxTheta[varNum, 1] <- mtxTheta[varNum, 1] - (alpha/ nObs) * vecPartial[varNum]
            }
        }
        
        # Update the cost vector, using slot intCtr + 1
        vecCost[intCtr + 1] <- (0.5 / nObs) * sum((gdX %*% mtxTheta - gdY)^2)
    }
    
    plot(vecCost, main=paste0("Cost Function vs. Iterations: ",alpha))
    print(paste0("Intercept: ",round(mtxTheta[1,1],4)))
    print(round(mtxTheta[-1,1] / sdX[-1],4))
    
    return(gdX %*% mtxTheta)
}

baseTime <- proc.time()

y0001 <- gdTest(alpha=0.0001)
y0003 <- gdTest(alpha=0.0003)
y0010 <- gdTest(alpha=0.001)
y0030 <- gdTest(alpha=0.003)
y0100 <- gdTest(alpha=0.01)
y0300 <- gdTest(alpha=0.03)
y1000 <- gdTest(alpha=0.1)
y3000 <- gdTest(alpha=0.3)

print("Total run time for function gdTest: ")
print(proc.time() - baseTime)

# Benchmarking based on subset of mtcars dataset
# mtxX is 32x6 (32 observations, each with an intercept plus 5 predictors)
# mtxY is 32x1 (32 results)
# nIter is 20,000
# 8 combinations of alpha are run, all that converge so there are no NaN or related timing issues
# 
# Run using matrix maths to simultaneosuly update theta:  4.3 user 2.1 system 6.9 elapsed
# RMSE from different alpha [lm() RMSE is 2.306]: 3.675 2.398 2.320 2.306 2.306 2.306 2.306 2.306
# 
# Run using for loops to simultaneously update theta:  14.1 user 2.1 system 17.7 elapsed
# RMSE from different alpha [lm() RMSE is 2.306]: 3.675 2.398 2.320 2.306 2.306 2.306 2.306 2.306
# 
# So, even with these very small datasets, matrix maths reduce user time by ~70% and elapsed time by ~60%
```
  
The gradient descents converge at different paces depending on the learning parameter (alpha).  For a look at the goodness of fit, we assess the actual predictions:  
```{r}
graphFrame <- cbind(y0001[,1], y0003[,1], y0010[,1], y0030[,1], 
                    y0100[,1], y0300[,1], y1000[,1], y3000[,1], 
                    mtcars$mpg
                    )
graphFrame <- graphFrame[order(graphFrame[,ncol(graphFrame)]),]
plot(x=1:nrow(graphFrame), y=graphFrame[,ncol(graphFrame)], type="l", 
     col="dark green", lwd=3, ylim=c(0,50)
     )
lines(x=1:nrow(graphFrame), y=graphFrame[,1], col="red", lty=2)
lines(x=1:nrow(graphFrame), y=graphFrame[,4], col="blue", lty=2)
lines(x=1:nrow(graphFrame), y=graphFrame[,6], col="purple", lty=2)
lines(x=1:nrow(graphFrame), y=graphFrame[,8], col="orange", lty=2)
```

As expected, the smaller alpha have not yet had enough time to converge, and so they have worse predictions (their gradients are still declining).  On the other hand, for the larger alpha, they have had enough time to converge without inducing oscillations, suggesting that the are a reasonable solution to the problem.  To assess how good, we examine:  
```{r}
rmseLM <- sqrt(mean((summary(mpgLM)$residuals)^2))
rmseMod <- rep(0, ncol(graphFrame)-1)
for (intCtr in 1:(ncol(graphFrame)-1)) {
    rmseMod[intCtr] <- sqrt(mean((graphFrame[,ncol(graphFrame)] - graphFrame[,intCtr])^2))
}

print(paste0("RMSE from LM: ",round(rmseLM,3)))
print(paste0("RMSE from Gradient Descent: "))
print(round(rmseMod,3))
```
  
Gradient descent eventually finds coefficients that are close to optimal for RMSE.  This exploration is helpful for seeing some advantnges of gradient descent (no matrix inverting means it can be fast for many predictors, no per se requirement that the cost function be linear), as well as some advantages of the closed form (converges immediately to the optimal answer with no iterations, easier to program).  This will be a good topic for continued exploration.  
  

###_Logistic Regression_  
Logistic regression ("logit") is used to make binary classifications.  By convention, 1 often expresses the presence of something (malignnancy, school acceptance, fraudulent transaction, etc.) while 0 is set to express the absence of that something.  The models work the same regardless of which state is called 1, of course.  
  
There are many drivers behind using logit rather than linear regression for a classification problem.  Among the drivers:  
  
* New data can play a large role even when they obviously do not matter.  For example, if we model tumor size vs. mailgnancy, we may find 2.5 cm is roughly where most tumors start to be malignant.  We might even get an OK linear regression line that passes through a prediction of 0.5 at ~2.5 cm.  But, if we add malignancies with 10 cm, the regression line will pass through 0.5 at a larger tumor size.  This makes little sense as our model had already learned that 2.5 cm is the cutoff; it became stupider when exposed to additional data that agreed with it!  
* The linear regression will also classify some points as being below 0 probability and above 1 probability.  While this is not the end of the world (we will predict <0 as 0 and >1 as 1), neither is it sensible.  

An example of this phenomenon is shown below, based on entirely made up data:  
```{r}
set.seed(1605181831)

par(mfrow=c(1, 2))

## Create some fake tumor data
tumorSize <- runif(100, min=0, max=0.1)
tumorType <- rbinom(length(tumorSize), 1, 1 - ((0.1 - tumorSize)^2 * 100))
fakeTumorLMOrig <- lm(tumorType ~ tumorSize)
keyCutOrig <- (0.5-coef(fakeTumorLMOrig)[[1]])/coef(fakeTumorLMOrig)[[2]]
predOrig <- pmax(0, pmin(1, round(predict(fakeTumorLMOrig, data.frame(tumorSize)), 0) 
                         ) 
                 )

plot(tumorSize, tumorType, xlab="Tumor Size", ylab="Malignant (1=Yes, 0=No)", col=1+predOrig, pch=19,
     main=paste0("50% Probability at Tumor Size: ",round(keyCutOrig,4)), cex.main=0.85
     )
abline(fakeTumorLMOrig, col="blue", lwd=2)
abline(h=0.5, v=keyCutOrig, lty=2, lwd=2)

confOrig <- confusionMatrix(predOrig, tumorType, positive="1")
text(0.04, 0.4, paste0("Accuracy: ",round(confOrig$overall[["Accuracy"]], 2)), adj=c(0, 0))
text(0.04, 0.34, paste0("Sensitivity: ",round(confOrig$byClass[["Sensitivity"]], 2)), adj=c(0, 0))
text(0.04, 0.28, paste0("Specificity: ",round(confOrig$byClass[["Specificity"]], 2)), adj=c(0, 0))
text(0.04, 0.22, paste0("+ Pred Val: ",round(confOrig$byClass[["Pos Pred Value"]], 2)), adj=c(0, 0))
text(0.04, 0.16, paste0("- Pred Val: ",round(confOrig$byClass[["Neg Pred Value"]], 2)), adj=c(0, 0))

## Add some larger tumors that are all malignant
tumorSizeMod <- c(tumorSize, runif(50, min=0.1, max=0.2))
tumorTypeMod <- c(tumorType, rep(1, 50))
fakeTumorLMMod <- lm(tumorTypeMod ~ tumorSizeMod)
keyCutMod <- (0.5-coef(fakeTumorLMMod)[[1]])/coef(fakeTumorLMMod)[[2]]
predMod <- pmax(0, pmin(1, round(predict(fakeTumorLMMod, data.frame(tumorSizeMod)), 0) 
                        ) 
                )

plot(tumorSizeMod, tumorTypeMod, xlab="Tumor Size", ylab="Malignant (1=Yes, 0=No)", col=1+predMod, pch=19,
     main=paste0("50% Probability at Tumor Size: ",round(keyCutMod,4)), cex.main=0.85
     )
abline(fakeTumorLMMod, col="blue", lwd=2)
abline(h=0.5, v=keyCutMod, lty=2, lwd=2)

confMod <- confusionMatrix(predMod, tumorTypeMod, positive="1")
text(0.04, 0.4, paste0("Accuracy: ",round(confMod$overall[["Accuracy"]], 2)), adj=c(0, 0))
text(0.04, 0.34, paste0("Sensitivity: ",round(confMod$byClass[["Sensitivity"]], 2)), adj=c(0, 0))
text(0.04, 0.28, paste0("Specificity: ",round(confMod$byClass[["Specificity"]], 2)), adj=c(0, 0))
text(0.04, 0.22, paste0("+ Pred Value: ",round(confMod$byClass[["Pos Pred Value"]], 2)), adj=c(0, 0))
text(0.04, 0.16, paste0("- Pred Value: ",round(confMod$byClass[["Neg Pred Value"]], 2)), adj=c(0, 0))

par(mfrow=c(1,1))

```
  
The LM appropriately responds to the new data by driving up the intercept (more of the data are positive, and making more positive predictions will drive up the overall accuracy).  But, this means we declare tumors of size 0.011 to have a 50% chance of being malignant, even though the closed has probability of malignancy at size 0.011 as ~20%.  
  
This can be seen from the very poor specificity of the modified test; we essentially declare everyone to have cancer, so a positive result from our test does not have much meaning (prior would not change by that much.
  
Logistic regression is an attempt to cure some of these deficiencies.  Broadly, differences in the logit include:  
  
* Predicted probability, h, in the logit is 1 / [ 1 + exp(-(X * theta)) ], as opposed to X * theta in linear regression.  The 1 / (1 + exp(-d)) is referred to as the sigmoid function, which has the desirable properties that it asymptotes to 0 for d of -Inf and to 1 for d of +Inf; it also is 0.5 for d=0, so making predictions can be as easy as just looking at each predicted d [-(X * theta) ] and using 0 as the cutoff  
* The cost function is defined based on the "true y" to be -log(h) for "true y == 1" and -log(1-h) for "true y == 0".  Essentially, the more extreme (whether towards 0 or towards 1) the probability decalred by the model, the greater the penalty for a missed prediction.  
* This cost function is asserted to be convex, while using sum((y-x)^2) as in linear regression is asserted to have many local minima.  Further, this cost function is asserted to be equivalent to a maximum likelihood estimate (MLE).  
* The short version seems to be that a great deal of thought (and proof) went in to this cost function, making it a good modelling choice  
  
The logit model is now tested on the same fake malignancy data as used in the previous example.  The call for the logit in R is based on glm() using family="binomial".  The prediction method is broadly the same, with an added call to type="response":  
```{r}
par(mfrow=c(1, 2))

## Run the GLM (logit) for the original tumor data
fakeTumorGLMOrig <- glm(tumorType ~ tumorSize, family="binomial")

## When response variables times coefficients equal intercept, h=0.5 (sigmoid(0))
keyCutOrigGLM <- -coef(fakeTumorGLMOrig)[[1]]/coef(fakeTumorGLMOrig)[[2]]
predOrigGLM <- pmax(0, pmin(1, round(predict(fakeTumorGLMOrig, data.frame(tumorSize), type="response"), 0) 
                           ) 
                    )

plot(tumorSize, tumorType, xlab="Tumor Size", ylab="Malignant (1=Yes, 0=No)", col=1+predOrigGLM, pch=19,
     main=paste0("50% Probability at Tumor Size: ",round(keyCutOrigGLM,4)), cex.main=0.85
     )
points(x=tumorSize, y=predict(fakeTumorGLMOrig, data.frame(tumorSize), type="response"), col="blue", pch=19)
abline(h=0.5, v=keyCutOrigGLM, lty=2, lwd=2)

confOrigGLM <- confusionMatrix(predOrigGLM, tumorType, positive="1")
text(0.04, 0.4, paste0("Accuracy: ",round(confOrigGLM$overall[["Accuracy"]], 2)), adj=c(0, 0))
text(0.04, 0.34, paste0("Sensitivity: ",round(confOrigGLM$byClass[["Sensitivity"]], 2)), adj=c(0, 0))
text(0.04, 0.28, paste0("Specificity: ",round(confOrigGLM$byClass[["Specificity"]], 2)), adj=c(0, 0))
text(0.04, 0.22, paste0("+ Pred Val: ",round(confOrigGLM$byClass[["Pos Pred Value"]], 2)), adj=c(0, 0))
text(0.04, 0.16, paste0("- Pred Val: ",round(confOrigGLM$byClass[["Neg Pred Value"]], 2)), adj=c(0, 0))

## Run the logit on the dataset with the larger tumors that are all malignant
fakeTumorGLMMod <- glm(tumorTypeMod ~ tumorSizeMod, family="binomial")

## When response variables times coefficients equal intercept, h=0.5 (sigmoid(0))
keyCutModGLM <- -coef(fakeTumorGLMMod)[[1]]/coef(fakeTumorGLMMod)[[2]]
predModGLM <- pmax(0, pmin(1, round(predict(fakeTumorGLMMod, data.frame(tumorSizeMod), type="response"), 0) 
                        ) 
                )

plot(tumorSizeMod, tumorTypeMod, xlab="Tumor Size", ylab="Malignant (1=Yes, 0=No)", col=1+predModGLM, pch=19,
     main=paste0("50% Probability at Tumor Size: ",round(keyCutModGLM,4)), cex.main=0.85
     )
points(x=tumorSizeMod, y=predict(fakeTumorGLMMod, data.frame(tumorSizeMod), type="response"), 
       col="blue", pch=19
       )
abline(h=0.5, v=keyCutModGLM, lty=2, lwd=2)

confModGLM <- confusionMatrix(predModGLM, tumorTypeMod, positive="1")
text(0.04, 0.4, paste0("Accuracy: ",round(confModGLM$overall[["Accuracy"]], 2)), adj=c(0, 0))
text(0.04, 0.34, paste0("Sensitivity: ",round(confModGLM$byClass[["Sensitivity"]], 2)), adj=c(0, 0))
text(0.04, 0.28, paste0("Specificity: ",round(confModGLM$byClass[["Specificity"]], 2)), adj=c(0, 0))
text(0.04, 0.22, paste0("+ Pred Value: ",round(confModGLM$byClass[["Pos Pred Value"]], 2)), adj=c(0, 0))
text(0.04, 0.16, paste0("- Pred Value: ",round(confModGLM$byClass[["Neg Pred Value"]], 2)), adj=c(0, 0))

par(mfrow=c(1,1))

```
  
While the overall accuracy of the logit is actually slightly lower (with the added malignancy data), the model is in aggregate substantially better.  Specifically, the model on the original data already tuned to predict 99%+ probabilities at sizes of 0.10 and above.  So, when the model gets a whack of new data that is all 100% malignant at sizes of 0.10 - 0.20, it does not learn anything new (it is, for practical purposes, unchanged).  As such, the model remains just as good at predicting negatives while getting better (technically, just seeing even more obvious cases of) at predicting positives.  It retains its specificity.  
  
It is notable that for "ideal" faked data (the original 100 data points), the LM slightly outperformed the logit.  The underlying data has structure that is neither precisely p ~ x nor precisely p ~ 1/(1+exp(-x))) so neither of these formulations precisely matches the data.  As always, the question during analysis is which model better reflects realtiy and better predicts unseen cases.  There is nothing magical about the logit that makes it certain to outperform simply because the outcomes are all 0/1.  But, it is a decent default with some very attractive properties.  
  
###_Regularization_  
The concept of regularization is that it can be helpful to shrink the magnitudes of the coefficients, largely to avoid overfitting to the training data.  The specific implementation is that that sum-squared-error for the linear regression becomes sum((f(x) - y)^2) + lambda * sum(theta^2), where by convention the intercept term is not included in the theta^2 summation.  
  
In general, regularization means worse predictions on the training data, since the method preferences small increases in RMSE if achieved by holding down the magnitude of the parameters.  However, the method can increase test/validation accuracy materially by vastly reducing variance (overfitting of the training data).  

A classic case for regularization is conofunding input vectors correlated to a relevant input vector.  While we "know" the correct model is Y = x1 + eps, if x2/x3 ~ x1 the regression can find something silly like Y = -2 * x1 + 4 * x2 - 1 * x3 + eps based on a small structure that happens to appear in the randomness in the training data.  The small gain in RMSE is completely artificial, and can fail in the validation stage.  For example:  
```{r}
set.seed(1605200803)
nSim <- 40

x1 <- rnorm(nSim)  #x1 = N(0, 1)
x2 <- 0.5 * x1 + rnorm(nSim, 0, 0.01)  # x2 will be 0.5 * x1 + N(0, 0.01)
x3 <- 2 * x1 + rnorm(nSim, 0, 0.4)  # x3 will be 2 * x1 + N(0, 0.4)
y <- x1 + rnorm(nSim)  # y will be x1 + N(0, 1)

# Test how well y = x1 fits the data - what is the R^2?
rndTSS <- sum( (y - mean(y))^2 )
rndSSE <- sum( (y - x1)^2 )
rndR2 <- 1 - rndSSE / rndTSS
print(paste0("R-squared for an assumed model of y = x1 is: ", round(rndR2,4)))

# Run the actual regression using lm
rndLM <- lm(y ~ x1 + x2 + x3)
summary(rndLM)
```

The full linear regression improves R-squared (unadjsuted) from 0.5255 to 0.5866, but at the expense of wildly varying thetas (+13.6 vs. +1.0 and -22.3 vs. 0.0).  It is reasonable to doubt this model will perform as well on an independent validation set - let's see:  
```{r}
x1Test <- rnorm(nSim)  #x1Test = N(0, 1)
x2Test <- 0.5 * x1Test + rnorm(nSim, 0, 0.01)  # x2Test will be 0.5 * x1Test + N(0, 0.01)
x3Test <- 2 * x1Test + rnorm(nSim, 0, 0.4)  # x3Test will be 2 * x1Test + N(0, 0.4)
yTest <- x1Test + rnorm(nSim)  # yTest will be x1Test + N(0, 1)

# Test how well y = x1 fits the test data - what is the R^2?
rndTestTSS <- sum( (yTest - mean(yTest))^2 )
rndTestSSE <- sum( (yTest - x1Test)^2 )
rndTestR2 <- 1 - rndTestSSE / rndTestTSS
print(paste0("R-squared when applying y = x1 to test data is: ", round(rndTestR2,4)))

# Test how well the lm fits the data
rndPredLM <- predict(rndLM, data.frame(x1=x1Test, x2=x2Test, x3=x3Test))
lmTestSSE <- sum( (yTest - rndPredLM)^2 )
print(paste0("R-squared when applying lm(y ~ x1 + x2 + x3) to test data is: ", 
             round(1-lmTestSSE/rndTestTSS, 4)
             )
      )
```

Absent regularization, the LM performs worse than the assumed y=x1.  While this is a contrived example, it provides an excellent illustration of how attempting to reduce bias with additional predictors can instead explode variance.  The "more accurate" training model then performs materially worse on validation data.  

A further issue in this example is that there is very high VIF in all of the LM coefficients, meaning that the model is rather unstable to small perturbations.  So, the coefficients and predictions not only have higher variance, they are additionally much harder to interpret.  See plots:  
```{r}
par(mfrow=c(1, 2))

plot(x=x1, y=y, pch=19, col="blue", main="Training data")
abline(a=0, b=1, lwd=2, lty=2, col="orange")
points(x=x1, y=predict(rndLM, data.frame(x1=x1, x2=x2, x3=x3)), pch=19, col="red", cex=0.75)

plot(x=x1Test, y=yTest, pch=19, col="blue", main="Testing data")
abline(a=0, b=1, lwd=2, lty=2, col="orange")
points(x=x1Test, y=rndPredLM, pch=19, col="red", cex=0.75)

par(mfrow=c(1, 1))
```
  
The red points represent an overfit to the training data, as they are (essentially) tuning on some noise in x2 and x3 while trying to keep x1 + x2 + x3 = 1 in line with the actual trend of the data (x1=1, x2=x3=0).  When the overfit is applied to the testing data, it is not certain to perform well; in this example, it is actually performing worse.  
  
